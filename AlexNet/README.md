# AlexNet

## Key Takeaways

* ReLu activation was used.

* Local Response Normalisation was used.

* Dropout was also introduced to battle overfitting.

## Model Architecture

![Arch](supplements/architecture.png)

![Arch](supplements/arch.png)
